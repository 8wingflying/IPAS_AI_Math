# 超參數優化
- GridSearchCV
- RandomizedSearchCV
- Bayesian Optimization
- **Optuna** | TPE（Tree-structured Parzen Estimator）
  - [Optuna - A hyperparameter optimization framework](https://github.com/optuna/optuna)
  - 201907 [Optuna: A Next-generation Hyperparameter Optimization Framework](https://arxiv.org/abs/1907.10902)
  - makes use of different samplers such as grid search, random, bayesian, and evolutionary algorithms
  - https://github.com/optuna/optuna-examples
  - https://www.geeksforgeeks.org/machine-learning/optuna/
  - https://zh-cn.optuna.org/
  - https://github.com/optuna/optuna-dashboard
  - https://github.com/optuna/optuna-mcp
- **Hyperopt** | TPE
  - [Hyperopt: Distributed Hyperparameter Optimization](https://github.com/hyperopt/hyperopt)
  - https://github.com/hyperopt/hyperopt

- https://www.geeksforgeeks.org/machine-learning/hyperparameter-tuning/

## 貝葉斯最佳化（Bayesian Optimization）
貝葉斯最佳化是一種專門用來解 **黑箱函數最佳化（Black-box Optimization）** 的方法，特別適用於：
- 函數不可微分
- 計算成本昂貴（如深度學習訓練）
- 沒有明確的解析式，只能透過實驗得到結果

它的核心精神是用 **機率模型（常為 Gaussian Process, GP）** 來代理未知的目標函數，並透過 **效用函數（Acquisition Function）** 來決定下一個最佳的採樣點。

---



#### Bayesian Optimization 流程
1. **初始化**：選定一些點，計算函數值  
2. **訓練代理模型（GP）**：使用現有資料來逼近真實函數  
3. **選擇採樣點**：使用 Acquisition Function  
4. **實際評估**：在新點上計算真實函數值  
5. **更新資料**：加入新數據再重建模型  
6. 重複直到收斂

---

#### Gaussian Process（高斯過程）
高斯過程是一種以：
- **均值函數**
- **協方差函數（Kernel）**

來建模不確定性的方式。

常見 kernel：
- RBF kernel
- Matern kernel
- Rational Quadratic kernel

---

#### Acquisition Functions（效用函數）
效用函數負責告訴我們下一步該嘗試哪個點：

### 常見類型：
- **EI（Expected Improvement）**：期望進步值  
- **PI（Probability of Improvement）**：改善的機率  
- **UCB（Upper Confidence Bound）**：上界置信分數

這些效用函數代表「探索（exploration）」與「開發（exploitation）」的平衡。

---

#### Python 實作：用 Bayesian Optimization 最小化一個函數

以下示例使用 `scikit-optimize (skopt)`。

```python
from skopt import gp_minimize
from skopt.plots import plot_convergence
import numpy as np
import matplotlib.pyplot as plt

# 目標函數（黑箱）
def f(x):
    return (x[0] - 3)**2 + np.sin(5 * x[0])

# 搜尋範圍
space = [(-2.0, 8.0)]

# Bayesian Optimization
res = gp_minimize(
    f,
    space,
    n_calls=30,
    random_state=42,
    acq_func="EI"  # Expected Improvement
)

print("最佳 x =", res.x)
print("最佳 f(x) =", res.fun)

# 收斂圖
plot_convergence(res)
plt.show()
```

---

# 2. 加入三大超參數最佳化工具比較  
本章重點比較：

| 工具 | 方法論 | 適合問題 | 主要特色 | 優點 | 缺點 |
|------|---------|-----------|------------|---------|---------|
| **Bayesian Optimization（skopt）** | GP Surrogate + Acquisition | 昂貴黑箱函數 | 經典 BO | 探索/開發平衡佳 | 高維度效果下降 |
| **sklearn + GridSearchCV / RandomizedSearchCV** | 非 BO | 高維可，但效能有限 | sklearn 內建且簡單 | 必要基礎搜尋方法 | 不適用昂貴函數 |
| **Optuna** | TPE（Tree-structured Parzen Estimator） | 高維、高噪音 | 自動化 pruning、分散式 | 現今最強超參數工具之一 | 不提供 GP surrogate |
| **Hyperopt** | TPE | 高維黑箱 | 老牌 TPE | 穩定、可整合 Spark | 尚未支援 richer trials |

---

# 3. sklearn Random / Grid Search 示範

```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
model = RandomForestClassifier()

param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [3, 5, None]
}

grid = GridSearchCV(model, param_grid, cv=5)
grid.fit(X, y)

print("最佳參數 = ", grid.best_params_)
```

---

# 4. Optuna 示範（TPE）

```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)

def objective(trial):
    n_estimators = trial.suggest_int("n_estimators", 50, 300)
    max_depth = trial.suggest_int("max_depth", 2, 12)

    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth
    )
    scores = cross_val_score(clf, X, y, cv=5)
    return -scores.mean()

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=50)

print("最佳參數:", study.best_params)
```

Optuna 特性：
- 自動 pruning  
- 快速 + 支援高維度  
- 適合模型調參、深度學習  

---

# 5. Hyperopt 示範（TPE）

```python
from hyperopt import fmin, tpe, hp, Trials
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)

space = {
    "n_estimators": hp.quniform("n_estimators", 50, 300, 1),
    "max_depth": hp.quniform("max_depth", 2, 12, 1)
}

def objective(params):
    clf = RandomForestClassifier(
        n_estimators=int(params["n_estimators"]),
        max_depth=int(params["max_depth"])
    )
    score = cross_val_score(clf, X, y, cv=5).mean()
    return -score

trials = Trials()
best = fmin(
    fn=objective,
    space=space,
    algo=tpe.suggest,
    max_evals=50,
    trials=trials
)

print("最佳參數:", best)
```

Hyperopt 特性：
- 穩定、老牌  
- 支援多種搜尋空間  
- Spark 分散式支援  

---

# 6. 三工具結果與差異總結

| 項目 | sklearn Grid/Random | Optuna | Hyperopt | Bayesian Optimization (skopt) |
|------|----------------------|---------|-----------|--------------------------------|
| 方法 | Grid / Random | TPE | TPE | GP-based BO |
| 高維 | 一般 | **強** | 強 | 中等 |
| 高噪音 | 普通 | **擅長** | 擅長 | 需調 kernel |
| 效能 | 中 | **最佳** | 佳 | 優 |
| 易用性 | 最簡單 | **非常簡單** | 還可 | 需理解 BO |
| 分散式 | 一般 | **支援** | 支援 | 一般 |

---

# 7. 比較分析

```python
import time
import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score

# Optuna
import optuna

# Hyperopt
from hyperopt import fmin, tpe, hp, Trials

# Skopt
from skopt import gp_minimize
from skopt.space import Integer

X, y = load_iris(return_X_y=True)

def evaluate_model(clf):
    scores = cross_val_score(clf, X, y, cv=5)
    return scores.mean()

results = {}

# ------------------------------------------------
# 1. Grid Search
# ------------------------------------------------
param_grid = {
    "n_estimators": [50, 100, 200],
    "max_depth": [2, 4, 6],
}

start = time.time()
clf = RandomForestClassifier()
grid = GridSearchCV(clf, param_grid, cv=5)
grid.fit(X, y)
results["grid"] = {
    "best_params": grid.best_params_,
    "best_score": grid.best_score_,
    "time": time.time() - start
}

# ------------------------------------------------
# 2. Random Search
# ------------------------------------------------
param_dist = {
    "n_estimators": np.arange(50, 300),
    "max_depth": np.arange(2, 12),
}

start = time.time()
rand = RandomizedSearchCV(
    clf, param_dist, n_iter=30, cv=5, random_state=42
)
rand.fit(X, y)
results["random"] = {
    "best_params": rand.best_params_,
    "best_score": rand.best_score_,
    "time": time.time() - start
}

# ------------------------------------------------
# 3. Optuna (TPE)
# ------------------------------------------------
def optuna_objective(trial):
    n_estimators = trial.suggest_int("n_estimators", 50, 300)
    max_depth = trial.suggest_int("max_depth", 2, 12)

    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth
    )
    return -evaluate_model(model)

start = time.time()
study = optuna.create_study(direction="minimize")
study.optimize(optuna_objective, n_trials=30)
results["optuna"] = {
    "best_params": study.best_params,
    "best_score": -study.best_value,
    "time": time.time() - start
}

# ------------------------------------------------
# 4. Hyperopt (TPE)
# ------------------------------------------------
space = {
    "n_estimators": hp.quniform("n_estimators", 50, 300, 1),
    "max_depth": hp.quniform("max_depth", 2, 12, 1),
}

def hyperopt_objective(params):
    model = RandomForestClassifier(
        n_estimators=int(params["n_estimators"]),
        max_depth=int(params["max_depth"])
    )
    return -evaluate_model(model)

start = time.time()
trials = Trials()
best = fmin(
    fn=hyperopt_objective,
    space=space,
    algo=tpe.suggest,
    max_evals=30,
    trials=trials
)
results["hyperopt"] = {
    "best_params": best,
    "best_score": -min(trials.losses()),
    "time": time.time() - start
}

# ------------------------------------------------
# 5. Skopt Bayesian Optimization (GP)
# ------------------------------------------------
space_skopt = [
    Integer(50, 300),
    Integer(2, 12),
]

def skopt_objective(params):
    n_estimators, max_depth = params
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth
    )
    return -evaluate_model(model)

start = time.time()
res = gp_minimize(
    skopt_objective,
    space_skopt,
    n_calls=30,
    random_state=42
)
results["skopt"] = {
    "best_params": {"n_estimators": res.x[0], "max_depth": res.x[1]},
    "best_score": -res.fun,
    "time": time.time() - start
}

# ------------------------------------------------
# 最終結果輸出
# ------------------------------------------------
for method, r in results.items():
    print(f"=== {method.upper()} ===")
    print("最佳參數:", r["best_params"])
    print("最佳分數:", r["best_score"])
    print("花費時間:", r["time"])
    print()
```

---

# 8. 結論
- 若模型訓練昂貴 → **Bayesian Optimization / Optuna / Hyperopt**
- 若模型訓練便宜 → **sklearn Grid / Random**
- 若需要自動剪枝、分散式 → **Optuna**

---
